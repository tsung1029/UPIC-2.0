2D MPI/OpenMP Particle-in-Cell (PIC) BEPS2 codes
by Viktor K. Decyk, UCLA
copyright 1994-2017, regents of the university of california

These codes are part of the UPIC 2.0.3 Framework.  The primary purpose
of this framework is to provide trusted components for building and
customizing parallel Particle-in-Cell codes for plasma simulation.
The framework provides libraries as well as 3 different reference
applications illustrating their use.  The libraries are designed to
work with up to 3 levels of parallelization (using MPI for distributed
memory, OpenMP for shared memory, local vectorization).  Currently only
the first two are supported.  The reference applications are designed to
provide basic functionality common to many PIC codes, and are intended
to be customized and specialized by users for specific applications.
Currently, only spectral field solvers are currently supported, but the
particle methods are designed to be general.  In addition, other test
codes will also be provided, illustrating the use of some specific
component.  This is primarily intended to users who wish to incorporate
some component into an already existing code. Currently, only three test
codes are provided.

A Software License for use of this code is in the document:
Documents/License(CommercialReservation)

A description of the design of these codes and its components are in the
documents:
Documents/BEPS2Design.pdf
Documents/MPI-OpenMP-PIC.pdf

A description of the current capabilities of these codes and its
components is in the document:
Documents/BEPS2Overview.pdf

Details about the mathematical equations and units used in this code is
given in the document:
Documents/UPICModels.pdf.

A description of the input parameters to these codes and their default
values are in the document:
mbeps2.source/input2mod.f90

There are 3 Fortran reference codes for electrostatic, electromagnetic,
and darwin models, called mpbeps2, mpbbeps2, and mpdbeps2, respectively.
They are designed for Linux but will also compile for Mac OS X.  OpenMP
and MPI are used for parallelization, but if MPI is not available, the
codes can be compiled for OpenMP only

There are also 3 Python scripts for electrostatic, electromagnetic, and
darwin models, called mbeps2.py, mbbeps2.py, and mdbeps2.py,
respectively.  They require 3 dynamic libraries, libmpush2.so,
libmbpush2.so, and libmdpush2.so, respectively, which are located in
mpbeps2.source.  These python scripts currently only work in single
precision and only with OpenMP.

A number of Directories are being developed to provide test codes to
illustrate components of these codes.  These are primarily intended for
those who wish to incorporate only pieces of the code into their own
codes.  The first one of these is the directory Periodic which
shows how to solve for the potential, longitudinal electric field,
vector potential and magnetic field using periodic boundary conditions.
Another one is the directory Dirichlet0, which shows how the solve
for the potential, longitudinal electric field, vector potential and
magnetic field using Dirichlet (conductinbg) boundary conditions.
A third one is the directory ReadFiles which contains sample programs
that illustrate how to read periodic fields which have been written
using simple Fortran IO by the reference applications.

To compile all these codes in the current (mpbeps2) directory, execute:

make

To compile just one, execute:

make program_name

where program_name is either: mpbeps2, mpbbeps2, or mpdbeps2

To execute, one first needs to copy an input file.  There are some
examples in the Examples directory in the current directory.  Files for
the electrostatic code are in the Examples/ES directory, those for the
electromagnetic code are in the Examples/EM directory, and those for the
darwin code are in the Examples/Darwin directory. To copy the the plasma
wave example for electrostatic code, type:

cp Examples/ES/input2.plasma input2

The command to execute a program with both MPI and OpenMP varies from
one system to another.  One possible command is:

mpiexec -np nproc -perhost n ./program_name

where nproc is the number of processors to be used, and where n is the
number of MPI nodes per host.

By default, OpenMP will use the maximum number of processors it can find
on the MPI node.  If the user wants to control the number of threads, a
parameter nvpp can be set in the main program.  In addition, the
environment variable OMP_NUM_THREADS may need to be set to the maximum
number of threads per node expected.  Finally, if OpenMP 3.1 is
available, setting the environment variable OMP_PROC_BIND=true generally
gives better performance by preventing threads from moving between CPUs.

To compile all these codes without MPI, execute:

make nompi

To compile just one, execute:

make program_name

where program_name is either: mbeps2, mbbeps2, or mdbeps2

To execute witout MPI, type the name of the executable:

./program_name

To delete compiled libraries (.o files) and the executables, execute:

make clobber

For more details about compilation, including use of double precision
and debug graphics, see mpbeps2.source/README

To compile the Python libraries in the current (mpbeps2) directory,
execute:

make python

To execute the scripts, then type:

python program_name

where program_name is either: mbeps2.py, mbbeps2.py, or mdbeps2.py

If files are generated by the reference applications, sample programs
to read them are available in the Directory ReadFiles.  Fortran programs
(preadf2, pvreadf2, pflreadf2, pfvreadf2, treadf2, and psreadf2) can be
compiled by executing:

make readf

These programs can be used to read scalar, vector, fluid, velocity, or
test particle trajectory or phase space data.

There are also 6 Python scripts available (preadf2.py, pvreadf2.py,
pflreadf2.py pfvreadf2.py, ptreadf2.py, and psreadf2.py).  They require
two dynamic libraries (libcmfield2.so and fgraf.so).  To compile the
libraries and copy the scripts, execute:

make pyreadf

The execute the scripts, type:

python scriptname

where scriptname is either: preadf2.py, pvreadf2.py, pflreadf2.py,
pfvreadf2.py ptreadf2.py or psreadf2.py

To delete the library and the files in the mbeps2 directory, execute:

make readfclobber


## Notes on Generating HDF5 output:

Currently Viktor dumps field/particle data in binary which can depend on the architecture (Big Endian/Little Endian) and the compiler.  We want to change that by making the code dump in a machine independent format, e.g., HDF5.  Currently, the field data is being dumped by the subroutine mpwrite2, mpwrite3, mpwrpart2(), mpwrpart3(), etc..  We want to replace these write statements with its HDF5 equivalent.

Fortunately, this has been done on upic-emma 2.0 and also a branch of QuickPIC.  We will port it to UPIC-2.0 as a branch for testing.
